<!doctype html>







































<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="en-us"
>
  <head><script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blog/livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>CS449 notes - Azure</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="0603 updated: https://juejin.cn/post/6844903553727725582" />
  <meta name="author" content="Axiwa" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="http://localhost:1313/blog/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="http://localhost:1313/blog/theme.png" />

  
  
  
  
  <link rel="preload" as="image" href="https://avatars.githubusercontent.com/u/23453707?s=400&amp;u=54da7b5e1e7d8acd4ddb973963f7d8c4a27bf860&amp;v=4" />
  
  

  
  
  <link rel="preload" as="image" href="http://localhost:1313/blog/github.svg" />
  
  <link rel="preload" as="image" href="http://localhost:1313/blog/rss.svg" />
  
  

  
  
  <script
    defer
    src="http://localhost:1313/blog/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  

  
  <link rel="icon" href="http://localhost:1313/blog/favicon.ico" />
  <link rel="apple-touch-icon" href="http://localhost:1313/blog/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.128.2">

  
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold"
      href="http://localhost:1313/blog/"
      >Azure</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6">
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="https://axiwa.github.io/"
        >Life</a
      >
      
    </nav>
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 dark:invert lg:ml-12 lg:mt-0 lg:items-center lg:space-x-6"
    >
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/axiwa"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
      <a
        class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./rss.svg)"
        href="http://localhost:1313/blog/index.xml"
        target="_blank"
        rel="alternate"
      >
        rss
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"
    >
      

<article>
  <header class="mb-16">
    <h1 class="!my-0 pb-2.5">CS449 notes</h1>

    
    <div class="text-sm antialiased opacity-60">
      
      <time>Apr 2, 2023</time>
      
      
      
      
    </div>
    
  </header>

  <section><p>0603 updated: <a href="https://juejin.cn/post/6844903553727725582">https://juejin.cn/post/6844903553727725582</a></p>
<hr>
<h2 id="lec-1-introduction">Lec 1 Introduction</h2>
<h3 id="scale-and-hyperscale">Scale and Hyperscale</h3>
<p>The ability of an architecture to scale appropriately as increased demand is added to the system. For a datacenter, scaling means increasing computing ability, memory, networking infrastructure, storage resources.</p>
<h3 id="scaling-up-and-scaling-out">Scaling up and scaling out</h3>
<p><strong>Scaling up</strong>: More power &amp; resources is added to existing machines, scaling <strong>vertically</strong>. Constant time as resources increase in propotion to increasing data size.</p>
<p><strong>Scaling out</strong>: Adding more machines to spread out the load, taking advantage of the cumulative capacity of shared resources. Scaling <strong>horizontally</strong>.</p>
<p><strong>Speed-up:</strong> Proportionally less time as resources increase for a given amount of data.</p>
<h3 id="data-intensive-applications">Data-intensive applications</h3>
<p>Characterized by the fact that they manipulate huge volume of data, potentially complex, changing, increasing, unable to be handled by a single computer and fit in memory.</p>
<h3 id="reliability">Reliability</h3>
<p><strong>Byzantine failures:</strong> Failures that are caused by arbitrary behaviour of some system component and there is imperfect information on whether a component has failed.</p>
<p><strong>Redundancy:</strong> Achieve system reliability. Backups, checkpointing, RAID, RDD lineage, etc.</p>
<h3 id="scalability">Scalability</h3>
<p><strong>Measure performance:</strong> Latency, response time, average versus percentiles (e.g. 95th), etc.
<strong>Tail-latency:</strong> A small percentage of responses of a service that takes more time that usual. It still may slow down the entire computation.</p>
<h3 id="clouds">Clouds</h3>
<p><strong>Elasticity:</strong> The ability to automatically adapt the employed resources to varying workload. The typical infrastructure choice for elastic systems is Cloud computing.</p>
<h2 id="lec-2-distributed-systems">Lec 2 Distributed systems</h2>
<h3 id="definition">Definition</h3>
<p>A system with multiple components (potentially heterogeneous) located on different machines that communicate and coordinate actions in order to <strong>appear as a single coherent system to the user</strong>. Resources of several machines are aggregated, the system is more scalable, faster, and reliable, more complex (no global clock, unpredictable failures of components, highly variable bandwidth, large latency &hellip;).</p>
<h3 id="properties">Properties</h3>
<p>Robustness</p>
<p>Availability: Are services and data always available to clients?</p>
<p>Scalability</p>
<p>Transparency: Be perceived as a whole system rather than a collection: <a href="https://superuser.com/questions/1263299/what-is-transparency-in-distributed-systems-in-simple-words">https://superuser.com/questions/1263299/what-is-transparency-in-distributed-systems-in-simple-words</a></p>
<p>Concurrency</p>
<p>Security</p>
<p>Efficiency</p>
<h3 id="centralized-architecture-client-server">Centralized architecture (client-server)</h3>
<p>One entity (the server) has a global view of the system. Example: <strong>Web</strong>. Simple, easier to control. Not reliable, lack of scalability.</p>
<h3 id="fully-distributed-architecture-p2p">Fully distributed architecture (P2P)</h3>
<ul>
<li>
<p>All participating entities (nodes) are both clients and servers and contribute the the system they use. There is no central node that has global knowledge of the system.</p>
</li>
<li>
<p><strong>The core: Overlay Networks</strong>: a virtual network abstraction specifying the topology of the P2P system and implemented on top of a physical network. Logical or virtual link between nodes corresponds to a path, perhaps through many physical links.</p>
</li>
<li>
<p>Single point of failure (SPOF) is a part system that if it fails, will stop the entire system from working.</p>
</li>
<li>
<p><strong>Topology</strong>:</p>
<ul>
<li>centralized: Every node is connected to other nodes, as well as a central sever</li>
<li>fully connected: No central entity</li>
<li>hybrid</li>
<li>Structured P2P. DHT-Based: Topology strictly determined by node IDs. Not all nodes have complete knowledge of all nodes.</li>
</ul>
</li>
<li>
<p><strong>Distibuted Hash Table</strong>:</p>
<ul>
<li>Autonomy and decentralization</li>
<li>Fault tolerance</li>
<li>Scalability
Each of nodes maintains a set of links (O(log n)) to other nodes. The stucture of a DHT includes keyspace, keyspace partitioning scheme, overlay network.</li>
</ul>
</li>
</ul>
<p>The identifier of a node is its ID (for instance, a SHA1 hash of the IP). Each object is assigned a 160-bit long identifier (key). Each node is responsible for a range of keys by specific algorithm.</p>
<ul>
<li>
<p>Self-organization</p>
</li>
<li>
<p><strong>Pastry</strong>:</p>
<ul>
<li>
<p>Each node is assigned a 128-bit long identifier. Place the node and the objects in a ring uniformly at random.</p>
</li>
<li>
<p>Each object is associated to the node whose identifier is the <strong>closest</strong> to the object identifier.</p>
</li>
<li>
<p>Despite concurrent node failures, eventual delivery is guaranteed unless [L/2] nodes with adjacent nodeIds fail simultaneously.</p>
</li>
<li>
<p><strong>Routing</strong></p>
<ul>
<li>In each routing step, a node normally forwards the message to a node whose nodeId shares with the key a prefix that is at least one digit longer than the prefix that the key shares with the present node&rsquo;s id.</li>
<li><strong>Routing table</strong>: route(key, msg)
<ul>
<li>routeTable(i, l): nodeId matching the current node identifier up to level i, with the next digit l (-rest of nodeIds)</li>
<li>Lazily repaired</li>
</ul>
</li>
<li><strong>Leaf set</strong>: 8 or 16 closest numerical neighbors in the naming space. Closest larger and smaller nodeIds. Leaf set is aggressively monitored and fixed.</li>
<li><strong>Neighborhood Set</strong>: contains the nodeIds and IP addresses of the M nodes that are closest to the local node (according to the <strong>proximity metric</strong>).</li>
<li>Algorithm on node A:</li>
</ul>
<pre tabindex="0"><code>IF (D i s wihin range of leaf set)
  DONE
ELSE
  check in the routing table
  IF (there is entry in the routing table, which shares the longer prefix with D than current node)
    msg is forwarded to that node
ELSE
  msg is forwarded to a node hsaring a prefix with the key at least as long as the local node, and is numerically closer to the key.
</code></pre><p>This procedure always converges.</p>
<ul>
<li>
<p><strong>Initialization</strong> of the routing table</p>
<ul>
<li>You have a node A to start from, which is already part of the system. The new nodeId is X. X asks A to route a special &ldquo;join&rdquo; message with the key equal to X, and Pastry routes the join message to the existing node Z whose id is numerically closest to X. All nodes along this path from A to Z send their state table to X. X inspects this information and initializes its own state tables, then informs any nodes that need to be aware of its arrival.</li>
<li>A is assumed to be in proximity to the new node X, A&rsquo;s neighborhood set to initialize X&rsquo;s neighborhood set. Z has the closest existing nodeId to X, thus its leaf set is the basis for X&rsquo;s leaf set.</li>
<li>Consider the condition where A and X share no common prefix. Then row 0 of A&rsquo;s routing table (A0) contains the appropriate value for X0, row 1 of B&rsquo;s (B is the first node encountered from A to Z) routing table is appropriate for X1, C2 is appropriate entries for X2, and so on.</li>
<li>Total cost is $O(log_{2^b}N)$</li>
</ul>
</li>
<li>
<p><strong>Departure</strong></p>
<ul>
<li>Failure or explicit departure</li>
<li>Nodes may fail or depart without warning. The failure of a node can be detected by another node that attempts to contact it during routing and there is no response. This may not affect the routing of a message, but a replacement entry must be found to preserve the routing table.</li>
<li>A node attempts to contact each member of the neighborhood set periodically to see if it is still alive.</li>
</ul>
</li>
<li>
<p><strong>Locolity</strong>: Reducing latency</p>
<ul>
<li>Filling the routing table whenever possible with nodes that are close geographically and satisfying the constraints of the routing table. (Populating the routing table)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="lec-3-cloud-computing">Lec 3 Cloud Computing</h2>
<h3 id="characteristics">Characteristics</h3>
<p>On-demand service</p>
<p>Ubiquitous network access</p>
<p>Location transparent resource polling</p>
<p>Rapid Elasticity</p>
<p>Measured service with pay per use</p>
<p>(Massive scal; Data-intensive; New programming paradigms&hellip;)</p>
<h3 id="flavours">Flavours</h3>
<p>Hardware as a service: your own cluster</p>
<p>Infrastructure as a service: flexible computing and storage infrastructure in the cloud. IaaS is fully self-service for accessing and monitoring computers, networking, storage, and other services. <a href="https://avinetworks.com/glossary/infrastructure-as-a-service-iaas/">https://avinetworks.com/glossary/infrastructure-as-a-service-iaas/</a></p>
<p>Platform as a service: provide cloud components to certain software while being used mainly for applications. All servers, storage, and networking can be managed by the enterprise or a third-party provider while the developers can maintain management of the applications.</p>
<p>Software as a service: SaaS utilizes the internet to deliver applications, which are managed by a third-party vendor, to its users.</p>
<h3 id="mapreduce">MapReduce</h3>
<p>batch computing</p>
<ul>
<li>
<p>Mapper</p>
<ul>
<li>Each mapper takes fraction of input (distibuted file system)</li>
<li>UDF</li>
<li>Output: key/value pair (local file system)</li>
</ul>
</li>
<li>
<p>Shuffling
*</p>
</li>
<li>
<p>Reducer</p>
<ul>
<li>Fetch files from every mapper and merge</li>
<li>UDF</li>
<li>Operate on all intermediate values associated with the same intermediate key</li>
</ul>
</li>
</ul>
<h2 id="lec4-gossip-based-computing">Lec4 Gossip-based computing</h2>
<h3 id="multicastgroup-communication">Multicast/Group communication</h3>
<p>A node of a distributed system wants to send a message to a group of other nodes in the system.</p>
<p>Problems:</p>
<ol>
<li>the latency of the message delivery</li>
<li>the load balancing among nodes</li>
<li>the resilience to failure</li>
</ol>
<h3 id="multicast-topology">Multicast Topology</h3>
<ul>
<li>Centralized
<ul>
<li>fastest</li>
<li>severe unbalance</li>
</ul>
</li>
<li>Tree-based (spanning tree)
<ul>
<li>load is relatively evenly balanced</li>
<li>fragile structure where the failure of the parent node will lead to the entire subtree failure</li>
<li>tree maintaince</li>
</ul>
</li>
<li>Chain-based
<ul>
<li>worst latency</li>
<li>bset load balancing</li>
<li>extremely fragile</li>
</ul>
</li>
</ul>
<h3 id="gossip-based-dissemination">Gossip-based dissemination</h3>
<p>Each node spreads a message to a sample of other nodes randomly (e.g. f (fanout) out of n members of the group). Each node reveiving a msg for the <strong>first</strong> time forwards it to f nodes chosen uniformly at random among n. Zr is the number of infected nodes prior to round r.</p>
<ul>
<li>
<p>Two approaches: Anti-entropy/gossip</p>
</li>
<li>
<p>atomic &ldquo;infection&rdquo;: p(Zr = n) = p(everyone gets a msg) = $e^{-e^{-c}}$ if f = log(n)+c. The broadcast is achieved in O(log(n)) hops. <a href="http://se.inf.ethz.ch/people/eugster/papers/gossips2.pdf">http://se.inf.ethz.ch/people/eugster/papers/gossips2.pdf</a></p>
</li>
<li>
<p>Pros&amp;Cons</p>
<ul>
<li>simplicity, emergent structure, convergence, robustness from replication</li>
<li>overhead, hard to cope with malicious behavior</li>
</ul>
</li>
<li>
<p><strong>Propagating information ways</strong></p>
<ul>
<li>Push: once you have a multicase message, start gossping</li>
<li>Pull: Periodically pull a few randomly selected nodes for new msg</li>
<li>Hybrid: First push, last pull</li>
</ul>
</li>
<li>
<p><strong>The peer sampling service</strong></p>
<p>Based on <a href="https://infoscience.epfl.ch/record/109297/files/all.pdf">https://infoscience.epfl.ch/record/109297/files/all.pdf</a></p>
<ul>
<li>An n node system; Each node (identified by IP) may join/leave/fail at any time;</li>
<li>Each node maintains <strong>a local view of c neighbors</strong> [IP, freshness];</li>
<li>Each node runs a <strong>passive (reactive)</strong> and an <strong>active(proactive)</strong> thread.</li>
<li><strong>Node selection</strong> =&gt; create an overlay network
<ul>
<li>Periodically each node selects another node as its peer according to a function <code>peerSelect()</code>. The sender exchange membership information with it.</li>
<li>Strategies: Rand/Head/Tail</li>
</ul>
</li>
<li><strong>Data exchange</strong>
<ul>
<li><code>viewPropagation()</code> function decides how nodes exchange their membership information, whether push/pull/push-pull</li>
<li><code>viewselection(c, buffer)</code> how view selection is performed. Select the information to exchange during the gossip operation.</li>
<li><code>buffer(h)</code>  [nodeIP, age] of neighbors and self. (Ignore the oldest h, c/2 first entries of the local view&hellip;)</li>
</ul>
</li>
<li><strong>Data processing</strong>
<ul>
<li>Once a node receives a buffer, the buffer is appended to its local view</li>
<li>Which entries and how many entries to be removed (h oldest) and kept (freshest)</li>
<li>s first items are removed: minimizing the correlation between the local views of the nodes</li>
<li>Random nodes may be removed</li>
<li>Strategies are determined by h (healer) and s(shuffle): random/healer/shuffle&hellip;</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Existing gossip-based membership system</p>
<ul>
<li>Lpbcast/Newscast/Cyclon</li>
</ul>
</li>
</ul>
<h2 id="lec7-consistency-models-and-protocols">Lec7 Consistency models and protocols</h2>
<p><a href="https://hellokangning.github.io/post/consistency-in-distributed-system/">https://hellokangning.github.io/post/consistency-in-distributed-system/</a></p>
<p>Most time and energy cost in computing is due to data movement</p>
<ul>
<li>Temporal locality and spatial locality</li>
<li>Locality can be exploited by: caching the data/prefetching/sequential access/patrtitioning the data among nodes. Some data structure are more appropriate than others to exploit locality</li>
<li>Partitioning is not applicable everywhere, e.g. graph (the smaller the cut, the better. Random graphs is resilient to parition)</li>
<li><strong>Consistency model</strong> describes the consistency contract between an application and a storage system. We need conistency model because there is data replicated.</li>
<li><strong>Schedules</strong>: serial schedule/equivalent schedule/serializable schedule&hellip;<a href="https://www.geeksforgeeks.org/types-of-schedules-in-dbms/">https://www.geeksforgeeks.org/types-of-schedules-in-dbms/</a></li>
</ul>
<h3 id="strict-consistency">Strict consistency</h3>
<ul>
<li>Any read on a data item X returns a value corresponding to the result of the most recent write on X.</li>
</ul>
<h3 id="strong-consistencylinearizability-sequential-consistency">Strong consistency(Linearizability, Sequential consistency)</h3>
<ul>
<li>A write to a variable does not have to be seen instantaneously, but must be in some sequential order. All memory operations need to happen in the program order</li>
<li>May produce non-deterministic results</li>
<li>Impossible to achieve in the presence of a partition (CAP), in an asynchronous system without assumptions on message delivery latencies (if I don&rsquo;t reveice response of one node for some time, I assume it has failed)</li>
</ul>
<h3 id="causal-consistency">Causal consistency</h3>
<ul>
<li>There are constraints only for causally related events. Only write operations that are causally related need to be seen in the same order by all processes</li>
<li>Monotonic reads consistency: Successive operation by the same process will return the same value, or more recent value.</li>
<li>Monotonic writes consistency: Any write happens before other writes on the same variable by the same processor can be seen by same processor</li>
<li>Read your writes: The effect of a write operation by a process on data x will be seen by successive read operation on x by the same process</li>
<li>Writes follow reads</li>
</ul>
<h3 id="eventual-consistency">Eventual consistency</h3>
<ul>
<li>Lack of simultaneous updates</li>
<li>If <strong>no update</strong> takes a very long time, all replicas eventually become consistent</li>
</ul>
<h3 id="newer-consistency-models">Newer consistency models</h3>
<ul>
<li>
<p>Per-key sequential</p>
</li>
<li>
<p>CRDT: Data structures for which commutated wrties give same result</p>
</li>
<li>
<p>A <strong>Protocol model</strong> describes the implementations of a consistency model</p>
</li>
</ul>
<h3 id="primary-based-protocols-centralized">Primary-based protocols: centralized</h3>
<ul>
<li>A primary server is responsible for every write operation</li>
<li>single point of failure</li>
<li>Write operation take place on the primary replica</li>
<li>Primary server is more loaded than others</li>
</ul>
<h3 id="replicated-write-protocols">Replicated write protocols</h3>
<ul>
<li>Write operation can be done on multiple replicas</li>
<li>They need to obey some ordering</li>
<li>Updates are sent to each replica in the form of an operation in order to be executed. All updates need to be performed in the same order in all replicas</li>
<li>Operation order can be managed by a totally-order multicast protocol (Lamport Timestamps) or a central coordinator</li>
</ul>
<h3 id="optimistic-reconciliation-protocols">Optimistic reconciliation protocols</h3>
<ul>
<li>Perform on data without synchronization and hope for the best</li>
<li>Manage consistency a posteriori</li>
</ul>
<h3 id="quorum-based-protocols">Quorum-based protocols</h3>
<ul>
<li>Quorum: a number of nodes (&gt;50%) that contain replicas of a given key-value pair.</li>
<li>Each client should acquire the permission of multiple servers before reading or writing a replicated data.</li>
<li>N: total replicas; R: read quorum; W: write quorum;</li>
<li><strong>Constraint</strong>: 1. R+W&gt;N 2. W&gt;N/2: Precluding concurrent write or write/read of two quorums</li>
<li>After the agreement, changes are applied on the file and <em>a new version number</em> is assigned to the updated file</li>
<li>Constraint 1 guarantees that a replica will not be read and written at the same time. Constraint 2 guarantees that a replica will no be written by 2 process at the same time.</li>
<li>(W=N, R=1) is good for read-heavy workloads. (W=1, R=N)/(w=N/2+1, R=N/2+1)</li>
</ul>
<h3 id="consensus">Consensus</h3>
<ul>
<li>
<p>Consensus problem: N processes outputs 0 or 1. How to design a protocol so that at the end, all variables output are 0 or 1). Goal: to have all processes decide same value</p>
</li>
<li>
<p>Constraint: Validity/Integrity/Non-triviality</p>
</li>
<li>
<p>FLP: It is impossible to differentiate a failed process with a slow one in an asynchronous distributed system.</p>
</li>
<li>
<p><strong>Synchronous/Asynchronous</strong> distributed system:</p>
<ul>
<li>Each message is received within bounded time/ No bounds</li>
<li>Drift of each process&rsquo; local clock has a known bound/ Arbitrary</li>
<li>Each step in a process takes lb &lt; time &lt; ub/ No bounds on message transmission delays</li>
</ul>
</li>
<li>
<p>In synchronous system, consensus is solvable. In the asynchronous system model, it is not possible. But in practice, it is possible</p>
</li>
<li>
<p><strong>Paxos</strong>:</p>
<p><a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/paxos-simple-Copy.pdf">https://www.microsoft.com/en-us/research/uploads/prod/2016/12/paxos-simple-Copy.pdf</a></p>
<ul>
<li><strong>safety</strong> thanks to the notion of majority, no two different values will be decided</li>
<li><strong>eventual liveness</strong> there will be eventually a unique leader able to decide</li>
<li>Each <strong>round</strong> has a unique ballot id. Rounds are synchronous.</li>
<li>If you are in round j and hear a message from round j+1, abort everything and move over to round j+1</li>
<li>Each round itself broken into <strong>3 phases</strong>: Leader selection -&gt; Leader proposes a value, processes ack -&gt; Leader multicasts final value</li>
</ul>
</li>
</ul>
<h2 id="lec5--lec6-nosql">Lec5 &amp; Lec6 NoSQL</h2>
<h3 id="three-database-revolutions">Three database revolutions</h3>
<ol>
<li>Navigational model</li>
<li>Relational model SQL
<ul>
<li>Logical data is disconneted from physical information storage</li>
<li>Transactions: sequence of operations</li>
<li>Users submit transactions, and can think of each transaction as executing by itself, no other users.</li>
<li>Each transaction must leave the system in consistent state. Concurrency is achieved by the DBMS, interleaving actions of various transactions</li>
</ul>
</li>
<li>NoSQL: <strong>large-scale dynamic</strong> distributed workload</li>
</ol>
<h3 id="acid-guarantees">ACID guarantees</h3>
<ul>
<li>Atomicity: all included statements in a trasaction are either executed or the whole transaction is aborted without affecting the database</li>
<li>Consistency: a database is in a consistent state before and after a transaction</li>
<li>Isolation: transactions can not see uncommited changes in the database</li>
<li>Durability: Changes are written to the disk before a database commits a transaction, so that committed data will not be lost through power failure</li>
</ul>
<h3 id="cap-theorem">CAP Theorem</h3>
<p>In a distributed system, you can not achieve these properties simultaneously, but <strong>at most two</strong></p>
<ul>
<li>Consistency: all users have the same view of the data</li>
<li>Availability: read/write reliably and quickly. To improve the availability, store data in more than one site or node</li>
<li>Partition-tolerence: still function normally during network partition.</li>
</ul>
<p><strong>Network partition is essential</strong>. Traditional RDBMS favors <strong>consistency</strong> while NoSQL favors <strong>availability</strong></p>
<h3 id="base-properties">BASE properties</h3>
<ul>
<li>Basic Availability</li>
<li>Soft-state: Copies of data may be inconsistent</li>
<li>Eventually consistent</li>
</ul>
<h3 id="key-value-data-model-kvs">Key-value data model (KVS)</h3>
<ul>
<li>A collection of key/value pairs</li>
<li>Distributed dictionary data structure, supporting insert, lookup and delete</li>
<li>Column-oriented storage: store and process by column, keeping all the data associated with a field next to each other in memory. Range searches are fast. <a href="https://dataschool.com/data-modeling-101/row-vs-column-oriented-databases/">https://dataschool.com/data-modeling-101/row-vs-column-oriented-databases/</a></li>
<li>Tables: &ldquo;Column families&rdquo; in Cassandra, &ldquo;Table&rdquo; in HBase, &ldquo;collection&rdquo; in MongoDB&hellip;but unstructured, no schema, no foreign key.</li>
</ul>
<h3 id="graph-data-model">Graph data model</h3>
<h3 id="cassandra"><strong>Cassandra</strong></h3>
<p><a href="https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf">https://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf</a></p>
<p><a href="https://medium.com/jorgeacetozi/cassandra-architecture-and-write-path-anatomy-51e339bcfe0c">https://medium.com/jorgeacetozi/cassandra-architecture-and-write-path-anatomy-51e339bcfe0c</a>  &lt;- Super clear!</p>
<p><a href="https://cassandra.apache.org/doc/latest/architecture/storage_engine.html">https://cassandra.apache.org/doc/latest/architecture/storage_engine.html</a> &lt;- Memtable and SSTable</p>
<p>Distributed storage system, large amount of structured data, no singel point of failure.</p>
<ul>
<li>C<strong>AP</strong>: Availability and partition tolerance</li>
<li>Columnbased NoSQL database, but the lowest level for key-value pairs is row</li>
<li>Different consistency levels</li>
<li>API: insert(table, key, row_mutation), get(table, key, columnName), delete(table, key, columnName)</li>
<li><strong>Data model</strong>:
<ul>
<li>Table: a distributed multi dimensional map indexed by a key</li>
<li>Row: associated to a unique key
<ul>
<li>Every operation under a single row key is atomic per replica</li>
</ul>
</li>
<li>Column: smallest data unit in Cassandra: name-value-timestamp</li>
<li>Supercolumn: column within a column</li>
<li>Column families: columns are grouped together into sets called column families. There are super column families and simple column families</li>
<li>Any column within a column family is accessed by &ldquo;column_familiy -&gt; column&rdquo; and any column within a super column family can be accessed by &ldquo;column_family -&gt; super_column -&gt; column&rdquo;</li>
<li>Keyspace: A namespace defines data replication on nodes. A cluster contains one keyspace per node</li>
</ul>
</li>
</ul>
<h3 id="cassandra-architecture">Cassandra Architecture</h3>
<ul>
<li>
<p><strong>Partitioning</strong></p>
<ul>
<li>All nodes participate in a cluster. They do not share anything</li>
<li>Consistent hashing, typically MD5-128: Each node is assigned a random value which represents its position on the ring. Each data identified by a key is also hashed to a position of the ring, and then assigned to a node by their position (chord-like). To address heterogeneity, load information is analyzed, have lightly loaded nodes move on the ring to alleviate heavily loaded nodes, or use virtual nodes. This node is in charge of this key</li>
<li>Departure or arrival affect only immediate neighbors</li>
<li>Each node is aware of every other node in the system</li>
<li>Partitioner: determines how data is distributed across the nodes in the cluster.</li>
</ul>
</li>
<li>
<p><strong>Replication</strong></p>
<ul>
<li>Each data item is replicated at N hosts, where N is the replication parameter. This achieves high-availability. The node stores keys that fall within its range, and also replicates these keys at the N-1 nodes in the ring.</li>
<li>Cassandra system elects a leader among its nodes using <strong>Zookeeper</strong> (Paxos variant). The leader tells all the nodes on joining the cluster what ranges they are replicas for and makes a concerted effort to maintain the invariant that <em>no node is responsible for more than N-1 ranges in the ring</em></li>
<li>Ranges a node is responsible for is cached locally at each node and in a fault-tolerant manner inside Zookeeper</li>
<li>Replication strategies: Simple Strategy vs Network Topology</li>
<li>Simple Strategy: RandomPartitioner assigns in a chord-like manner (clockwisely choosing next N-1 nodes to act as replicas), ByteOrderedPartitioner ranges of keys to servers</li>
<li>Network Topology Strategy: for multi-datacenter deployments</li>
</ul>
</li>
<li>
<p><strong>Write</strong></p>
<ul>
<li>
<p>Writes are atomic at row level</p>
</li>
<li>
<p>When a client sends request to a node, the node is the coordinator. Coordinatore uses partitioner (partition policy) to send query to one or all replica nodes responsible for key.</p>
</li>
<li>
<p>When x replicas respond, the coordinator returns an acknowledgement to the client.</p>
</li>
<li>
<p><strong>Hinted Handoff mechanism</strong>: If a replica is down, the coordinator writes to all other replicas, and keep the write locally until down replica comes back up. When all replicas are down, the coordinator buffers writes for up to a few hours, or throws OverloadedException</p>
</li>
</ul>
</li>
<li>
<p><strong>Read</strong>
<a href="https://marikalam.medium.com/study-guide-cassandra-data-consistency-496e5bf9cadb">https://marikalam.medium.com/study-guide-cassandra-data-consistency-496e5bf9cadb</a>
<a href="https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlAboutReads.html">https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlAboutReads.html</a></p>
<ul>
<li>The coordinator sends request to replicas, waits for enough responses and return the latest value to the client</li>
<li>At a replica, check memtable-&gt; row cache-&gt; bloom filter-&gt; partition key cache-&gt; compression offset map/partition summary.</li>
<li>Consistency is checked. Once a read is done, the coordinator compares the data from all remaining replicas for the given key. If they are consistent, the coordinator sends a write request to the out of date replica to update the row to most recently value.</li>
<li>Maybe slower than write but still fast</li>
</ul>
</li>
<li>
<p>Delete</p>
<ul>
<li>No item deletion right away. Add a tombstone to the log, and will be deleted at the point when compaction recognize this</li>
</ul>
</li>
<li>
<p>One ring per datacenter, one per-DC coordinator is elected to coordinate with other DCs.</p>
</li>
</ul>
<h3 id="cassandra-data-structures">Cassandra Data Structures</h3>
<ul>
<li>Partitioning key
<ul>
<li>It helps with determining which node in the cluster the data should be stored</li>
</ul>
</li>
<li>Commit log
<ul>
<li>The write request is appended to the commit log in the disk</li>
</ul>
</li>
<li>Memtable
<ul>
<li>A write-back cache. Cassandra looks up by key</li>
<li>The write request is sent to the memtable (in memory)</li>
<li>When the global memory threshold has been reached or commit log is full, the data is flushed to a SSTable on disk (sequential writes), and the data in the commit log is purged, memtable is marked as flushed.</li>
<li>Can be searched by key</li>
<li>Append-only datastructure</li>
</ul>
</li>
<li>SSTables-Sorted String Table
<ul>
<li>Index structure called <strong>bloom filter</strong>: <a href="https://llimllib.github.io/bloomfilter-tutorial/">https://llimllib.github.io/bloomfilter-tutorial/</a>
<ul>
<li>Compact way of representing a set of items, checking for existence in set is cheap</li>
<li>A propabilistic data structure. Maybe false positive, never false negative</li>
</ul>
</li>
<li>Compaction
<ul>
<li>merging all the updates associated to a given key periodically (multiple SSTables into one SSTable). Latest timestamp wins</li>
</ul>
</li>
</ul>
</li>
<li>Row cache
<ul>
<li>A memory cache which stores recently read rows (records)</li>
</ul>
</li>
<li>Partition indexes
<ul>
<li>sorted partition keys mapped to their SSTable offsets. Partition Indexes are created as part of the SSTable creation and resides on the disk.</li>
</ul>
</li>
</ul>
<h3 id="membership----gossip-based">Membership &ndash; Gossip based</h3>
<ul>
<li>Anti-entropy gossip based mechanism</li>
<li>Full membership, everyone knows each other.</li>
<li>Any server in cluster could be the coordinator. Every server needs to maintain a list of all the other servers.</li>
<li>Nodes keep a heartbeat count for each member and periodically gossip their membership list to neighbours, and then update it: Consistency of node network.</li>
<li><strong>Failure detection</strong>:
<ul>
<li>A mechanism by which a node can locally determine if any other node in the system is up or down. In Cassandra, also used to avoid attempts to communicate with unreachable nodes.</li>
<li>A modified version of the $\Theta$ Accrual Failure Detector: emits a value indicating the suspicion level whether the monitored node is down. PHI determines the detection timeout.</li>
<li>Adaptively set the timeout based on underlying network, historical inter-arrival time and failure behavior</li>
</ul>
</li>
</ul>
<h3 id="consistency">Consistency</h3>
<p>ANY (fastest) / ALL (strong consistency) / ONE / QUORUM (global)</p>
<h3 id="bootstrapping">Bootstrapping</h3>
<h2 id="lec8-scheduling">Lec8 Scheduling</h2>
<h3 id="framework-and-scheduler">Framework and scheduler</h3>
<p>Collections of anything from development tools to middleware to database services that manages and runs multiple cloud applications. No single framework is optimal for all applications, and it&rsquo;s difficult to run each framework on its dedicated cluster.</p>
<p>Goal of scheduling: 1. Good throughput or response time for tasks/jobs; 2. High utilization of resources. + Computing scheduling: Running multiple framework on a single cluster</p>
<h3 id="single-core-scheduling">Single-core scheduling</h3>
<ul>
<li>FIFO: average completion time may be high.</li>
<li>STF: Optimal solution wrt average completion time. Not fairest</li>
<li>Round-Robin: Requires to preempt processes at the end of each period and save their state to resume them later.</li>
<li>&hellip;</li>
</ul>
<p>FIFO and STF are better for batch applications, while Round-Robin is better for interactive applications.</p>
<h3 id="mesos">Mesos</h3>
<p><a href="https://zhuanlan.zhihu.com/p/21408890">https://zhuanlan.zhihu.com/p/21408890</a>
Mesos acts as an intermediary between the nodes and the framework, to accomodate multiple frameworks. It decouples the resource allocation and the task scheduling, but not takes all the requirement and resource as input to give global optimal scheduling.</p>
<ul>
<li>Global scheduler: can achieve optimal but too complex and to handle future new framework</li>
<li>Distributed scheduler: Master sends resource offers to frameworks: vector of available resources (e.g. 1CPU; 1GB). Frameworks select which offers to accept and which tasks to run</li>
<li><strong>Mesos architecture</strong>
<ul>
<li>Mesos master</li>
<li>Mesos slave: executor</li>
<li>Zookeeper: elect master</li>
<li>Frameworks: Scheduler + Executor</li>
</ul>
</li>
<li>Master knows where the slaves are and what resources the slave have. Slaves continuously send status updates about resources to the master.</li>
<li>Two level scheduling: Master sends resource offer to the scheduler of the framework (framework send their requirement to the master also). The framework scheduler selects resources and provides tasks. The executors of the framework launch tasks.</li>
<li>Highly scalable, easy to customize, small Mesos Codebase</li>
</ul>
<h3 id="resource-allocation">Resource allocation</h3>
<ul>
<li>Max-min (weighted) fairness: first allocate average, then if a user wants less than its fair share he will get less</li>
<li>Users have no reason to lie to ask for more resources.</li>
<li>For multiple cores: <strong>Dominant Resource Fairness</strong>
<ul>
<li>Equalize dominant shares</li>
<li><img src="../../images/cs449.png" alt=""></li>
</ul>
</li>
</ul>
<h2 id="lec9-stream-processing">Lec9 Stream Processing</h2>
<p><img src="../../images/cs449-2.png" alt=""></p>
<h3 id="problem">Problem</h3>
<p>Disseminate streams of events from various producers to various consumers. Data stream is unbounded data, broken into a sequence of individual tuples.</p>
<h3 id="messaging-system">Messaging System</h3>
<ul>
<li>
<p>Direct Messaging:</p>
<ul>
<li>Necessary in latency critical applications.</li>
<li>Both consumers and producers have to be online at the same time</li>
<li>Issues: comsumer can crash; Producers may send meesages faster than the consumers can process</li>
</ul>
</li>
<li>
<p><strong>Pub/sub system</strong></p>
<ul>
<li>A set of Subscribers/consumers and Publishers/producers</li>
<li>Pub-sub system: manages users subscripions, matches published events against subscriptions, disseminate events</li>
<li>Expressiveness: Topic-based (application-level multicast)/Content-based (attributed-based; range queries)</li>
<li><strong>Publisher</strong>: generate event data and publishes them</li>
<li><strong>Subscriber</strong>: submit their subscriptions and process the events received</li>
<li><strong>P/S service</strong>: the mediator/broker that filters and routes events from publishers to interested subscribers</li>
<li>Decoupling in time, space and synchronization</li>
<li>Centralized architecture: one centralized broker, subscribers/publishers do not need to know each other</li>
<li>Distributed architecture: a set of nodes act as brokers</li>
<li>Decentralized architecture: each node can be pub/sub/broker, communicating directly, they should have knowledge of each other</li>
</ul>
</li>
</ul>
<h3 id="key-function">Key function:</h3>
<ul>
<li>Event Filtering &amp; Event routing
<ul>
<li>Topic based: Each event is published to one of the channels. Subscribers subscribe to a particular channel and receive <strong>all</strong> events published to the subscribed channel
<ul>
<li>Event routing is heavily-loaded</li>
</ul>
</li>
<li>Content based: Allowing more expresson in the query. Event publication by a key/value attribute, and subscriptions specify filters using an explicit subscription language</li>
</ul>
</li>
</ul>
<h3 id="kafka">Kafka</h3>
<p>Partitioned log-based message broker, distributed stream processing software, topic oriented</p>
<ul>
<li>Partitioned log
<ul>
<li>A log is an append-only sequence of records on disk, so ordered. But ordering is only guaranteed within a partition for a topic</li>
<li>A producer sends message by appending it to the end of the log</li>
<li>A consumer receives messages by reading the log sequentially, in the order the messages are sorted in the log.</li>
<li>Messages in the log can be stored for a while</li>
<li>Logs are partitioned hosted on different machines</li>
<li>Each partition can be read and written independently of others</li>
<li>Within each partition, the broker assigns a monotonically increasing sequence number (offset) to every message</li>
<li>No ordering guarantee across partitions</li>
<li>Partitioned logs can be replicated or not on different partition</li>
</ul>
</li>
<li>Partitions of a topic are replicated: fault-tolerent
<ul>
<li>One broker is the leader of one partition. Read and write go through the leader</li>
<li>Zookeeper manages consistency: 1) detecting the addition and the removal of brokers and consumers; 2) Keeping trak of the comsumed offset of each partition</li>
<li>Followers passively replicate the leader for fault tolerance. Once a leader fails, one of the followers will assume the role of the leader</li>
</ul>
</li>
<li>State and guarantees
<ul>
<li>Brokers have no metadata for consumers-producers</li>
<li>Consumers are responsible for keeping track of offsets</li>
<li>Messages in queues expire based on the pre-configured time periods</li>
<li>Kafka guarantees that messages from a single partition are delivered to a consumer in order</li>
<li>Kafka only guarantees at-least-once delivery (the client needs to check for duplicate)</li>
</ul>
</li>
</ul>
<h3 id="streaming-data">Streaming Data</h3>
<ul>
<li>Processing patterns
<ul>
<li>Batch processing</li>
<li>Micro-batch: set of bounded data from unbounded data</li>
<li>Continuous procesing-based system</li>
</ul>
</li>
<li>Windowing
<ul>
<li>Window is a buffer associated with an input port to retain previously received tuples</li>
<li>Tumbling window: fixed length, every event belongs to <strong>excatly one window</strong> (no-overlapping), based on the happening time.</li>
<li>Hopping window: fixed length, allowing overlap</li>
<li>Sliding window: supports incremental operation, contains all events that occur within some interval and remove old events when they expire.</li>
<li>Session window: No fixed duration. Grouping together all events from the same user that occur closely in time. Frequently used for web site analysis (e.g. the clicks of a user in some Web page)</li>
<li>Windowing by processing time/event time (handling out-of-order events)</li>
</ul>
</li>
</ul>
<h3 id="streaming-processing-system">Streaming processing system</h3>
<ul>
<li>Processing element (PE): A PE is the basic functional unit in an application: inputs tuples, applies a function, and outputs tuples</li>
<li>A set of PEs and stream connections, organized into a data flow graph</li>
<li>PE state:
<ul>
<li>stateless tasks: do not maintain state, process each tuple independently of prior history: total parallelism, no synchronization</li>
<li>stateful tasks: maintains information across different tuples to detect complex patterns</li>
</ul>
</li>
<li>At runtime, an application is represented by one or more jobs. Jobs are deployed as a collection of PEs.</li>
<li>Logical plan vs. physical plan: Logical plan is a data flow graph, where the vertices correspond to PEs, and the edges to stream connections; Physical plan is a data flow graph where the vertices correspond to OS process, and the edges to transport connections</li>
<li>Parallelization: pipelined/task/data</li>
<li>How to allocate data items to computation instance: broadcast, shuffle, key-base&hellip;</li>
</ul>
<h2 id="lec10-distributed-learning">Lec10 Distributed learning</h2>
<h3 id="parameter-server">Parameter server</h3>
<p>To launch ML algorithms on a typical Cloud infrastructure, going beyond typical cluster-compute system</p>
<p>Parameters are stored in a distributed database (e.g. KVS) accesible through the network.</p>
<ul>
<li>Architecture
<ul>
<li>Server nodes
<ul>
<li>maintain a partition of the globally shared parameters</li>
<li>communicate with each other to replicate or migrate parameters</li>
<li><strong>perform bookeeping and global aggregation steps</strong></li>
</ul>
</li>
<li>Worker nodes
<ul>
<li><strong>perform computation</strong></li>
<li>store locally a portion of the training data</li>
<li>communicate with server nodes to update and retrieve the shared parameters</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="distributed-gradient-descent">Distributed Gradient Descent</h3>
<ul>
<li>Workers get the assigned training data</li>
<li>Workers <strong>pull</strong> the working set of model</li>
<li>Different parts of the model may be on different servers</li>
<li>Iterate until Stop:
<ul>
<li>Workers <strong>compute</strong> gradients</li>
<li>Workers <strong>push</strong> gradients</li>
<li>Servers <strong>aggregate</strong> into current model</li>
<li>Workers <strong>pull</strong> updated model</li>
</ul>
</li>
<li><img src="../../images/cs449-3.png" alt=""></li>
<li>Idealy, different workers follow the distribution of the data, as homogeneous as possible. But this will slow down the system</li>
</ul>
<h3 id="shared-parameters-key-value-vectors">Shared parameters: key-value vectors</h3>
<p>Model parameters are represented as key-value pairs (i, wi), but the semantics used by the server may be of vector or matrix. This enables to linear algebra operation and data locality.</p>
<h3 id="range-push-and-pull">Range Push and Pull</h3>
<p>Caller of the push and pull are always the workers. Range of keys can minimize network traffic. Non blocking operations: the caller inserts its requests in a queue and resume computation</p>
<h3 id="asynchronous-execution-and-flexible-consistency-model">Asynchronous execution and Flexible consistency model</h3>
<p>How to deal with inconsistency? <strong>Stale synchronous parallel (SSP)</strong></p>
<ul>
<li><strong>Bounded delay</strong>: x-bounded delay means the workerss (at time t) can tolerate which level out-of-date parameters (<strong>can&rsquo;t be older thatn t-x</strong>). 1-bounded means sequential, infnity means fully asynchronous.</li>
<li>Delayed SGD: trade-off between efficiency vs. synchronization</li>
<li><strong>Vector clocks</strong>: attached for each k-v pair for: tracking aggregation status, rejecting doubly sent data, recovery from failure&hellip;.<a href="https://lrita.github.io/2018/10/24/lamport-logical-clocks-vector-lock/">What is vector clock</a>, <a href="https://lrita.github.io../../images/posts/distribution/p558-lamport.pdf">Lamport timestamp</a></li>
<li>As many k-v pairs get updated at the same time during one iteration, they can share the same clock stamps, which reduces the space requirement</li>
<li>Consistent Hashing &amp; Replication
<ul>
<li>Use of DHT range partitioning</li>
<li>Servers are hashed in the ring</li>
<li>Server nodes store a replica of k-v pairs on k nodes counter clockwise to it.</li>
</ul>
</li>
</ul>
<h3 id="federated-learning"><strong>Federated Learning</strong></h3>
<p>Let the data stays where it is produced. Model parameters will never contain more information than the raw training data. Instead of uploading the raw data, train a model locally and upload the model.</p>
<p>In practice:</p>
<ul>
<li>Each round, a random fraction C of clients are selected by the server. Each client that has $n_k$ training data samples in federated learning ~ a randomly selected sample in traditional learning (C=1 means full-batch gradient descent)</li>
<li>The server sends model to clients, clients send updates to the server, the server aggregates, then sends back new parameters</li>
<li>Suppose n samples; K clients; $n_k$ data samples on the client k; $\eta$ learning rate; central server broadcasts current model $w_t$ to the selected clients, then
<ul>
<li>training objective: $min_{w}f(w)$</li>
<li><img src="../../images/cs449-4.png" alt=""></li>
<li>Each client k computed gradient: $g_k = \nabla F_k(w_t)$</li>
</ul>
</li>
<li>Depends on:
<ul>
<li>Fraction of the clients</li>
<li>Number of passes a client makes on its local data</li>
<li>Local mini-batch size used for the client update</li>
</ul>
</li>
</ul>
<h2 id="lec11-decentralized-learning">Lec11 Decentralized Learning</h2>
<h2 id="lec1213-graph-mining">Lec12&amp;13 Graph mining</h2></section>

  
  

  
  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="http://localhost:1313/blog/posts/cs440/"
      ><span class="mr-1.5"></span><span>CS440 Rendering Engine</span></a
    >
    
    
    <a
      class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="http://localhost:1313/blog/posts/rotation/"
      ><span>Euler Angle - Quaternion - Axis Angle</span><span class="ml-1.5"></span></a
    >
    
  </nav>
  
  

  
  

  
  

  


  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2024
    <a class="link" href="http://localhost:1313/blog/">Azure</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugo</a
  >
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    > Paper</a
  >
</footer>

  </body>
</html>
